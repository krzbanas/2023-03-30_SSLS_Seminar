---
title: "Classification of the Residues after High and Low Order Explosions"
subtitle: "using Machine Learning Techniques on Fourier Transform Infrared (FTIR) spectra"
author: "Krzysztof Banas<br><span style = 'font-size: 80%;'>SSLS<br></span>"
date: 2023-03-30
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countdown: 60000
      countIncrementalSlides: true
      titleSlideClass: ["center", "middle", "inverse"]
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = FALSE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#1381B0",
  secondary_color = "#FF961C",
  inverse_header_color = "#FFFFFF"
)
```

```{r xaringan-Extra, include=FALSE, warning=FALSE}
library(xaringanExtra)
xaringanExtra::use_scribble()
xaringanExtra::use_tile_view()
xaringanExtra::use_progress_bar(color = "#0051BA", location = "top")
```



class: inverse center middle

# Introduction

---


## SPF Project

.pull-left[

*   in collaboration with SPF
*   high and low order explosions
*   debris collected
*   analysed with infrared spectroscopy: FTIR spectrometer
]

.pull-right[


![Test](FIGURES/explosion.png)
]

---
## Experiments

.pull-left[

- KBr pellets
- mid-IR range
- multiple spectra measured
- spectral signatures from high energetic material
- ... but also from the surface material (paper, plastic, textile)
]

.pull-right[


]

![Test](FIGURES/table_spectra.png)

---
## Motivation for this sub-project

.pull-left[

*   check which materials are more suitable as sample collectors
*   check what is the influence of the order of explosion on the classification 
*   investigate dimension reduction method in combination with discrimination
*   building classification model: combined PCA-LDA

]

.pull-right[
![Test](FIGURES/LDA_02.png)


]


---
class: inverse center middle

# Results

---

## Spectral Data
.pull-left[

*   typically more variables than observations
*   variables are highly correlated and auto-correlated
*   before modeling need for dimension reduction
*   pre-processing is crucial
]

.pull-right[

![Test](FIGURES/raw_spectra.png)
]
---

## Pre-processing
.pull-left[

![Test](FIGURES/raw_spectra.png)
]

.pull-right[

![Test](FIGURES/norm_spectra.png)
]

---
class: inverse center middle
## Dimension Reduction

---
## How PCA works


![Test](FIGURES/PCA_2d_to_1d.png)]


---
## How PCA works

![Test](FIGURES/pca-anim.webp)]


---
## PCA on decathlon dataset
```{r}
library("FactoMineR")
library("factoextra")
library("DT")
data(decathlon)
#knitr::kable(head(decathlon2[, 1:9], n=8), format = 'html')
datatable(decathlon)
```


---
## Correlation between variables

```{r}
decathlon.active <- decathlon[1:13, 1:10]
cor.mat <- round(cor(decathlon.active),2)
library("corrplot")
corrplot(cor.mat, type="upper", order="hclust", 
         tl.col="black", tl.srt=45)
```



---
## Principal Components: screeplot and scoreplot

```{r}
res.pca <- PCA(decathlon.active, graph = FALSE)
eigenvalues <- res.pca$eig

datatable(format(eigenvalues, digits=2))
```

---

```{r}
barplot(eigenvalues[, 2], names.arg=1:nrow(eigenvalues), 
       main = "Variances",
       xlab = "Principal Components",
       ylab = "Percentage of variances",
       col ="steelblue")
# Add connected line segments to the plot
lines(x = 1:nrow(eigenvalues), eigenvalues[, 2], 
      type="b", pch=19, col = "red")
```

---
## Correlation between a variable and a principal component 

```{r}
var <- get_pca_var(res.pca)
datatable(format(var$coord, digits=2))
```


---
## Variable Correlation Plot
```{r, fig.retina=3, fig.height=4}
fviz_pca_var(res.pca, col.var="black")+theme_bw()
```

---
## Quality of Representation (cos<sup>2</sup>)
```{r}
datatable(format(var$cos2, digits=3, scientific=FALSE))
```


---
## Correlation Plot
.pull-left[
```{r, fig.height=6, fig.retina=3}
library(corrplot)
corrplot(var$cos2,is.corr=FALSE, cl.pos="r")
```
]

.pull-right[
```{r, fig.height=6, fig.retina=3}

fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
             )
```

]

---
## Contributions of variables to PC1
```{r}
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
```

---
## Contributions of variables to PC2
```{r}
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
```


---
## Contributions of variables to PC1 and PC2
```{r}
fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)
```




---
## Principal Components: scoreplot
```{r, fig.height=4}
fviz_pca_ind(res.pca)
```


---
## PCA: Practical Applications

- Finance: analyze stock market data to identify patterns 
- Image compression: compress digital images 
- Genetics: analyze genetic data to identify patterns
- Social media: social media data to identify trends in user behavior and preferences
- Marketing: consumer data to identify patterns in purchasing behavior and preferences
- Healthcare: patient data to identify risk factors and predict outcomes
- Quality control: manufacturing data to identify patterns in product quality and performance
- Agriculture: crop data to identify patterns in yields and optimize farming practices
- Climate research: climate data to identify patterns in weather patterns and predict future climate trends
- Sports: player data to identify key factors that contribute to winning games and championships.



---
## PCA for spectrosopic data
.pull-left[
- dimension reduction
- removing correlation between variables
- finding patters
- unsupervised method
- *lossy technique*
- *scaling variables can yield different results*
- *problems with interpretation: PCs are linear combination of original features*
]
.pull-right[

]

---
## Spectral dataset
```{r}
spectra01 <- read.csv("data01.csv")

datatable(spectra01[,-c(1,4)])
```

---
## Loadings and contribution plot
.pull-left[
![Test](FIGURES/loadings_spectra.png)
]
.pull-right[
- first two PCA
- all wavenumbers
- colour coded contribution cos<sup>2</sup> contribution to PC1 and PC2
]


---
## Scoreplots

.pull-left[
![Test](FIGURES/scoreplot_spectra.png)
]
.pull-right[
- first two PCA
- all observations (spectra)
- colour coded class membership (explosive and order)
]


---
class: inverse center middle
## Identification/Classification/Grouping

---
## 2D partitions
<img src="FIGURES/partition_plot.png"
     width="800px"
     style="position:absolute; left:10px; top:130px;">
---
## Building Classification Model

-  first: use PCA on spectral data for Dimension Reduction
-  then: use LDA to build the Classification Model using a first few PC's
-  finally: test the Classification Model with new data
---
background-image: url("FIGURES/LDA_00.png")
background-position: 50% 50%
background-size: 40% 60%
## How LDA works<sup>1</sup>





.footnote[[1] Source: Linear discriminant analysis : a detailed tutorial Gaber, T, Tharwat, A, Ibrahim, A and Hassanien, AE AI Communications (2017)]




---
## Data Preparation for LDA

- **Classification Problems** applied to classify the categorical output variable (suitable for binary and multi-class classification problems)
- **Gaussian Distribution** The standard LDA model applies the Gaussian Distribution of the input variables. One should review the univariate distribution of each variable and transform them into more Gaussian-looking distributions (use log and root for exponential distributions and Box-Cox for skewed distributions)
- **Remove Outliers** It is good to firstly remove the outliers from your data because these outliers can skew the basic statistics used to separate classes in LDA, such as the mean and the standard deviation
- **Same Variance** As LDA always assumes that all the input variables have the same variance, hence it is always a better way to firstly standardize the data before implementing an LDA model. By this, the mean will be 0, and it will have a standard deviation of 1.

---
class: inverse center middle
## LDA on principal components

---
## Discriminant functions plot
.pull-left[
![Test](FIGURES/LDA_01.png)
]
.pull-right[
- first two linear discriminant functions
- all observations (originaly spectra) 
- colour coded class membership (explosive and order)
]



---
## Discriminant functions plot
.pull-left[
![Test](FIGURES/LDA_02.png)
]
.pull-right[
- first two linear discriminant functions
- all observations (originaly spectra) 
- colour coded class membership (explosive and order)
]

---
## Comparison

.pull-left[
### PCA
- unsupervised
- maximize the variance in the given dataset
- good performer for a comparatively small sample size

]
.pull-right[
### LDA
- supervised
- find the linear discriminants to represent the axes that maximize separation between different classes
- more suitable for multi-class classification tasks
]

---
class: inverse center middle

# Conclusions

---
## Take home message

- Spectroscopic data are multidimensional and highly correlated
--

- PCA "transfers" most variability of the system to the first few principal components
--

- PCA removes correlation between variables
--

- For classification model we need another method: LDA
--

- With PCs as input variables LDA works well in predicting the class membership
--

- We can validate model with various statistical methods, for example Leave-One-Out


---
## Where to find:

.pull-left[

### Manuscript:

![Paper](FIGURES/banas2023a.html.png)
]


.pull-right[

### Slides: 
https://krzbanas.github.io/2023-03-30_SSLS_Seminar

![Seminar](FIGURES/seminar_title.png)

]


---
class: center, middle

# Thank You!

Slides created via the R packages:

[**xaringan**](https://github.com/yihui/xaringan)<br>
[gadenbuie/xaringanthemer](https://github.com/gadenbuie/xaringanthemer)

The chakra comes from [remark.js](https://remarkjs.com), [**knitr**](http://yihui.name/knitr), and [R Markdown](https://rmarkdown.rstudio.com).
